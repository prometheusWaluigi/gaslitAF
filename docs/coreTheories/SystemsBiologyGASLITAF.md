A Systems Biology Model for Chronic Multi-System Syndromes (GASLIT-AF Framework)

Introduction

Chronic Multi-System Syndromes (CMSS) such as ME/CFS, Long COVID, POTS, hypermobile EDS, fibromyalgia, and MCAS are complex conditions involving dysregulation across multiple physiological systems. They often manifest as persistent fatigue, pain (allodynia), autonomic imbalance, and immune dysfunction. We propose a unified GASLIT-AF framework – Genetic Autonomic Structural Linked Instability Theorem – Allodynic Fatigue – to model these syndromes as a nonlinear dynamical system. In this view, CMSS pathology emerges from instabilities in an interconnected network of genetic predispositions, autonomic regulation, structural integrity, and immune/metabolic feedback loops. The goal of this proposal is to formalize the GASLIT-AF model with rigorous mathematics, providing a state-space representation, differential equations with delays and noise, parameter definitions, and analysis tools. By framing CMSS as a dynamical disease (i.e. a disorder of system dynamics), we can leverage systems biology, nonlinear dynamics, and information theory to understand how multiple subsystems collectively produce chronic pathology ￼ ￼.

We proceed by defining the dynamical system architecture (state space, variables, and governing equations) and detailing the roles of key parameters (γ, Λ, Ω, σ, Θ, Ξ). We then analyze the stability and attractor landscape of the system via bifurcation theory, Lyapunov stability, and potential energy landscapes. Next, we integrate information-theoretic and thermodynamic measures (mutual information, entropy production, coherence, and complexity) to quantify system behavior. We extend the core framework with stochastic modeling (SDEs), network theory (graph representation of interactions), control theory (modeling interventions), and information geometry (analysis of the parameter manifold). Finally, we illustrate how specific biological pathways – for example, a KLK15 gene variant and the IDO metabolic trap – can be mapped onto model parameters, modulating feedback strengths and tipping the system into a diseased attractor. Throughout, we outline how this formal model can be validated and used as a basis for simulation-based hypothesis testing and therapy design.

Dynamical System Architecture

State Space and Variables

We define a state space Ψ that encompasses the relevant physiological variables across multiple systems. For modeling CMSS, Ψ is high-dimensional, representing multi-organ dynamic states. Each state variable $x_i(t)$ corresponds to a measurable or latent quantity in one subsystem. For example, the state vector $\mathbf{x}(t) = [x_1(t), x_2(t), \dots, x_n(t)]^T$ could include:
	•	Immune activation/inflammation level ($I(t)$) – e.g. cytokine concentrations or an index of inflammatory activity.
	•	Autonomic nervous system tone ($A(t)$) – e.g. sympathetic vs parasympathetic balance or heart rate variability index.
	•	Endocrine/metabolic state ($E(t)$) – e.g. cortisol level (HPA axis output) or cellular energy (ATP) availability.
	•	Central nervous system sensitization ($N(t)$) – e.g. a measure of pain signaling amplification or microglial activation in the brain.
	•	Structural integrity ($S(t)$) – e.g. connective tissue stability, reflecting susceptibility to microinjuries (relevant in hEDS).
	•	Mast cell activation ($M(t)$) – e.g. histamine or tryptase release level indicating allergic/MCAS activity.

These variables are highly interdependent; together they form the composite state $,\mathbf{x} \in \Psi$. We assume $\Psi$ can be treated as a continuous state space (e.g. $\mathbb{R}^n$ or a bounded subset if physiological ranges are considered) for the purpose of modeling. Healthy baseline functioning corresponds to a point or attractor in Ψ where all variables lie in normal ranges, whereas CMSS patients operate in an altered region of Ψ characterized by dysregulated values (e.g. chronically elevated inflammation, autonomic imbalance, etc.).

Nonlinear ODEs with Delayed Feedback and Noise

We postulate that the time-evolution of the state $\mathbf{x}(t)$ is governed by a set of nonlinear ordinary differential equations (ODEs) with feedback delays and stochastic noise terms. The general form of the GASLIT-AF dynamical model is:

$$
\frac{d\mathbf{x}}{dt} = F!\Big(\mathbf{x}(t),, \mathbf{x}(t-\tau_1),, \mathbf{x}(t-\tau_2),, \dots;; \boldsymbol{\Theta}\Big);+; \boldsymbol{\eta}(t),,
$$

where $F: \Psi \times \Psi \times \cdots \to \mathbb{R}^n$ is a nonlinear vector-valued function encoding the interactions among variables. The terms $\mathbf{x}(t-\tau_k)$ represent time-delayed feedback, with delays $\tau_k > 0$ accounting for finite response times in physiological loops (e.g. hormonal feedback delays, neural signal transmission delays, etc.). The parameter set $\boldsymbol{\Theta}$ (Greek Theta here denotes the collection of all model parameters, not to be confused with the specific parameter $\Theta$ defined later) determines the strength and form of interactions. Finally, $\boldsymbol{\eta}(t)$ is a noise term representing stochastic fluctuations (intrinsic noise from random molecular events, or extrinsic perturbations from the environment). In a more compact SDE-like notation, one could write $d\mathbf{x} = F(\mathbf{x}, \mathbf{x}_{delay}; \boldsymbol{\Theta}),dt + G(\mathbf{x};\boldsymbol{\Theta}),dW_t$, where $W_t$ is a standard Wiener process and $G$ shapes the noise covariance.

Each component equation $\dot{x}i = F_i(\mathbf{x}(t), \mathbf{x}(t-\tau{i1}), \ldots)$ captures how subsystem $i$ evolves based on the current state and past states of possibly other subsystems. The nonlinear couplings allow for complex behaviors such as feedback loops (positive or negative) and threshold effects. For example, an ODE for inflammation $I(t)$ might include a positive feedback term from itself with a delay (modeling cytokine auto-amplification or immune memory) and a negative feedback from cortisol (HPA axis effect), plus a noise term for random immune triggers. Symbolically, a toy example could be:

$$
\dot{I}(t) = -\gamma_I I(t);+; \Lambda_I \frac{I(t-\tau_I)^p}{1 + I(t-\tau_I)^p};-; \Omega_I A(t);+; \eta_I(t),,
$$

where $-\gamma_I I$ provides damping, the middle term is a sigmoidal feedback with delay $\tau_I$ and Hill coefficient $p$ modeling cytokine positive feedback that saturates, and the $-\Omega_I A(t)$ term represents autonomic (sympathetic) suppression of inflammation (with $\Omega_I$ as a coupling coefficient). This is just an illustrative form; the actual $F_i$ for each variable can be tailored to known biology. Delay differential equations (DDEs) like the above can generate oscillations and even chaos for certain parameters, as classic models (e.g. the Mackey-Glass equation for blood cell regulation) have shown ￼. Including delays is crucial: many CMSS patients experience oscillatory symptoms (circadian swings, “flare and crash” cycles) that simple ODEs without delays might not capture.

The noise term $\eta_i(t)$ can be modeled as Gaussian white noise with intensity $\sigma_i$, or a colored noise if specific correlation structure is needed. Noise represents unpredictable fluctuations (e.g. random allergen exposures causing mast cell activation, or stochastic variations in neuronal firing). Such stochasticity can push the system from one regime to another, or induce variability around an equilibrium. We will discuss the stochastic extension in a later section; initially, we focus on deterministic dynamics (setting $\eta(t)=0$) to analyze the system’s inherent stability structure.

Parameters and Their Roles in System Dynamics

The GASLIT-AF model involves key parameters denoted by Greek letters (γ, Λ, Ω, σ, Θ, Ξ). These parameters quantify physiological rates, feedback strengths, and thresholds. We rigorously define each parameter and explain its role:
	•	$\gamma$ (Gamma): Gamma represents damping rates or intrinsic decay constants for variables. In each ODE, $\gamma_i > 0$ would be the linear coefficient that pushes $x_i$ back toward baseline (homeostasis) in absence of other inputs. For example, $\gamma_I$ in the inflammation equation damps inflammation level (e.g. cytokine clearance rate). Higher γ means stronger restoring force towards equilibrium, generally promoting stability. Conversely, smaller γ implies slower recovery and can predispose to oscillations or instability if feedback drives growth faster than decay. Changes in $\gamma$ can lead to bifurcations: if damping is too low relative to driving forces, a stable fixed point can lose stability (e.g. a Hopf bifurcation if a pair of complex eigenvalues crosses into positive real part). Thus, $\gamma$ parameters critically influence stability margins and the system’s ability to recover from perturbations.
	•	$\Lambda$ (Lambda): Lambda typically denotes growth rates or feed-forward gains in the system. Here, $\Lambda$ represents parameters that scale activating feedback loops or baseline production rates of certain variables. For instance, $\Lambda_I$ in the example above scales the positive feedback in inflammation. More generally, $\Lambda$ could be a matrix or set ${\Lambda_{ij}}$ that multiplies terms where one variable activates another. An increase in a λ parameter might drive a variable to escalate more rapidly, potentially pushing the system into a new regime. In linear stability terms, $\Lambda$ often appears in the Jacobian matrix and contributes to eigenvalues: a higher $\Lambda$ can push an eigenvalue from negative to positive (signifying loss of stability). In our context, $\Lambda$ captures the propensity for runaway processes – e.g. the tendency of autonomic arousal to further increase itself or of immune response to amplify. Bifurcations such as saddle-node (turning on a new steady state) can occur when $\Lambda$ crosses a threshold value, suddenly enabling a self-sustaining high-activity state.
	•	$\Omega$ (Omega): Omega represents oscillatory dynamics parameters, often associated with frequencies or coupling in cyclic processes. In the model, $\Omega$ may parameterize the strength of coupling between subsystems that produce oscillations or rhythmic behavior. For example, $\Omega_I$ above couples autonomic tone to inflammation; if this coupling creates an effective negative feedback loop with a delay, a Hopf bifurcation can occur at a certain $\Omega$ where sustained oscillations emerge. $\Omega$ could also denote natural frequencies of certain oscillators (e.g. if we included an equation for circadian rhythm or heart rate oscillations, $\Omega$ would set the base frequency). Thus, $\Omega$ influences cycle timing and coherence among variables. Varying an $\Omega$ can move the system from steady state to limit cycle oscillations (when a Hopf bifurcation is passed) or change the period of existing oscillations. In summary, $\Omega$ controls rhythmic feedback properties and can be tuned to see if periodic symptom flare-ups correspond to limit-cycle attractors.
	•	$\sigma$ (Sigma): Sigma denotes noise intensity or variability. In a deterministic model $\sigma$ might be zero, but in the stochastic extension $\sigma$ scales the amplitude of $\eta(t)$. Each variable could have its own $\sigma_i$ or we may have a noise covariance matrix $\Sigma$. A higher $\sigma$ means larger random fluctuations. Noise can destabilize a system by knocking it out of a shallow attractor basin or enabling stochastic resonance and random switching between states. Interestingly, $\sigma$ can induce effective reductions in stability (e.g. a system that is barely stable might frequently transition to symptoms due to random shocks if $\sigma$ is high). Conversely, in some cases a bit of noise can stabilize oscillations via coherence resonance. In our framework, $\sigma$ will be important for simulating the unpredictability of symptom fluctuations in CMSS. It does not directly shift equilibrium values (that’s the job of other parameters), but it influences how often and how far the state strays from equilibrium. We will analyze entropy production associated with $\sigma$ in the thermodynamics section.
	•	$\Theta$ (Theta): Theta represents threshold levels in nonlinear responses. Many physiological processes have threshold behavior (e.g. a minimal stimulus is needed to trigger mast cell degranulation, or a certain level of ATP depletion triggers an alarm response). In equations, Θ might appear inside sigmoidal or step functions. For example, a term like $\frac{I}{1+e^{-(x-\Theta)}}$ would imply that when $x$ exceeds Θ, the response sharply increases. In our model, each feedback loop could have a threshold $\Theta_{ij}$ at which variable $j$ significantly affects $i$. Lowering a threshold means the system becomes more sensitive (e.g. allodynia can be modeled as a lowered pain threshold in neural circuits). Increasing a threshold can make a feedback dormant until more extreme conditions. The role of Θ in stability: if a threshold is very low, the system may engage strong feedback even for small deviations, potentially causing oscillation or bistability. If a threshold is high, the system might remain in a linear regime until a big perturbation pushes it past the threshold, possibly causing a sudden state change (like a hard switching nonlinearity). We include Θ to capture piecewise dynamics and hysteresis (path-dependent outcomes). Mathematically, a changing Θ can lead to catastrophe-like behavior where a small change in Θ shifts the system from one attractor to another by altering the nonlinear vector field discontinuously.
	•	$\Xi$ (Xi): Xi denotes coupling structure parameters, often represented as a matrix $\Xi_{ij}$ that weights the influence of variable $j$ on $i$. In network terms, $\Xi$ is the adjacency or connectivity matrix of our system graph. For example, $\Xi_{I,A}$ might quantify how strongly inflammation influences autonomic function, $\Xi_{A,I}$ the reverse, and so on. These could correspond to physiological pathways (e.g. $\Xi_{I,A}$ could represent vagus nerve-mediated reflex suppression of inflammation). The $\Xi$ parameters define the wiring of the network – which subsystems strongly affect others. While $\Lambda$ and $\Omega$ scale the overall feedback magnitude and oscillatory component, $\Xi$ determines the topology of interactions. Variations in $\Xi$ (such as removal or addition of a connection, or making it stronger/weaker) can qualitatively change system behavior by enabling new feedback loops or decoupling parts of the system. For instance, if a genetic predisposition or epigenetic change effectively increases a coupling $\Xi_{pain,ANS}$ (pain influencing autonomic arousal), then pain signals might trigger autonomic changes more strongly, potentially forming a vicious cycle. Stability analysis wise, $\Xi$ populates the Jacobian matrix off-diagonal terms; certain patterns of $\Xi$ can create strongly connected components that as a whole go unstable if net feedback product exceeds damping (per graph theory, this relates to the eigenvalues of the weighted adjacency matrix). We will later interpret specific biological links (like KLK15 or IDO) as affecting certain $\Xi$ entries.

All parameters together $\boldsymbol{\Theta} = {\gamma_i,; \Lambda_{ij},; \Omega_k,; \sigma_l,; \Theta_m,; \Xi_{pq}}$ define a point in parameter space. Healthy physiology corresponds to a region of this parameter space where the system’s attractors correspond to normal stable functioning. CMSS patients may have genetic or environmental factors that shift parameters into a region where the system has a pathological attractor or is on the edge of instability. In later sections, we will consider the “parameter manifold” and how curvature or sensitivity in this space relates to model behavior (an information geometry perspective).

Stability and Attractor Landscape Analysis

Equilibria and Local Stability (Bifurcation Theory)

A crucial step is to identify equilibria (steady states) of the ODE system: points $\mathbf{x}^$ such that $F(\mathbf{x}^, \mathbf{x}^*, \dots; \boldsymbol{\Theta}) = 0$. These equilibria represent candidate long-term states of the system, which could correspond to a patient’s baseline (either healthy or chronic ill state). To analyze stability, we examine the Jacobian matrix $J = \partial F/\partial \mathbf{x}$ evaluated at an equilibrium. The eigenvalues ${\lambda}$ of $J$ (not to be confused with parameter $\Lambda$) indicate local stability: if all have negative real parts, the equilibrium is locally stable (an attractor); if any eigenvalue has positive real part, the equilibrium is unstable. The parameters $\boldsymbol{\Theta}$ influence $J$; as parameters change, eigenvalues can pass through zero or acquire nonzero imaginary parts, leading to bifurcations.

We will use bifurcation theory to map out how changing a parameter can create or destroy attractors. For example, if we vary an immune gain parameter $\Lambda_I$, we might find a critical value where a pair of complex conjugate eigenvalues crosses the imaginary axis, indicating a Hopf bifurcation and the birth of a stable limit cycle (chronic oscillations in symptoms). Or varying a threshold $\Theta$ might cause a saddle-node bifurcation: two equilibria (one stable, one unstable) collide and annihilate, which could correspond to the sudden onset of a chronic illness state once a threshold is passed. Indeed, one hypothesis in ME/CFS (the IDO metabolic trap) suggests exactly this type of bistability: a mathematical model of tryptophan metabolism exhibited both a physiological steady state and a pathological steady state, and beyond a critical threshold in tryptophan levels, the system falls into the pathological steady state, from which escape requires an exogenous perturbation ￼. In general, our model may exhibit multistability – coexistence of multiple attractors (see Figure below) – and hysteresis, where the path into illness involves a different route than the path out.

To systematically analyze stability, we will apply:
	•	Linearization and eigenvalue analysis: derive conditions on $\gamma, \Lambda, \Xi, \dots$ for which the real parts of eigenvalues change sign. This yields bifurcation boundaries.
	•	Center manifold and normal form analysis: near a bifurcation (e.g. Hopf or saddle-node), reduce the system to a simpler form to characterize the nature of the bifurcation (subcritical vs supercritical Hopf, etc.), which has implications for how symptoms emerge (gradual oscillations vs explosive onset).
	•	Numerical bifurcation tools: use software (MATCONT, XPPAuto, or custom Python) to continue equilibria as parameters vary and detect bifurcations automatically. This will produce diagrams showing regions of stable healthy behavior vs chronic oscillation vs alternate stable states. Such analysis addresses the “instability theorem” part of GASLIT-AF: we hypothesize there is a parametric instability that underlies the transition to chronic illness. We will seek to prove or demonstrate (in simulation) that under certain parameter constraints, the healthy state loses stability and the system is inevitably drawn into a debilitating regime (allodynic fatigue attractor).

Attractor Landscape and Potential Wells

Beyond local stability, we consider a global attractor landscape – a conceptual landscape where valleys represent attractors (stable states or cycles) and hills/ridges represent unstable states or separatrices (boundaries between basins of attraction). One useful approach is to construct a potential function $V(\mathbf{x})$ if possible. If the system were gradient-based ($\dot{\mathbf{x}} = -\nabla V(\mathbf{x})$), then $V$ acts like a Lyapunov function and its minima are attractors. Our system is more complex (likely non-gradient due to cyclic feedbacks), but one can often approximate or visualize an “effective potential” in reduced dimensions. For instance, consider a one-dimensional caricature of healthy vs diseased states: a double-well potential.

Conceptual double-well potential landscape for a simplified system. The diagram illustrates two stable attractor states (valleys) – one on the left labeled “Healthy State” and one on the right labeled “Disease State” – separated by a potential barrier (peak) ￼. In this picture, an individual’s condition corresponds to a ball in one of the valleys. Small perturbations jostle the ball but it remains in its valley if the barrier is high. A large perturbation (or parameter change that lowers the barrier) can push the system over into the other valley (a disease onset or a recovery). This landscape metaphor helps conceptualize remission and relapse: e.g. an aggressive intervention might supply the “exogenous perturbation” needed to escape the pathological valley ￼, whereas persistent stress might deepen the pathological valley (making spontaneous escape unlikely).

Analyzing the attractor landscape can be done with several tools:
	•	Lyapunov functions and energy methods: We will attempt to find a Lyapunov function $V(\mathbf{x})$ for subsets of the system (or for the whole system under certain symmetric assumptions). A Lyapunov function that has local minima at equilibria proves their stability. In complex systems, finding an explicit Lyapunov function is challenging, but one might construct composite energies (e.g. an “error energy” for each subsystem and sum them). If a Lyapunov function exists for the healthy equilibrium, that indicates a strong resilience; loss of such a function or violation of its conditions would signal a qualitative change in dynamics.
	•	Direct simulation and basins of attraction: By simulating the ODEs with different initial conditions, we can map out basins of attraction. For example, varying initial inflammation or autonomic state and seeing where the system converges (healthy vs diseased equilibrium) gives an empirical basin boundary. This boundary often corresponds to an unstable manifold of a saddle equilibrium. Knowledge of the basin geometry is important for therapy: it tells us how big a push is needed to get from one state to another.
	•	Potential landscape approximation: In cases where stochastic dynamics are considered, one can compute a quasi-potential $U(\mathbf{x})$ via solving the stationary distribution (from the Fokker-Planck equation) or using methods from large deviation theory. The stationary distribution $P_{ss}(\mathbf{x}) \propto e^{-U(\mathbf{x})}$ defines an effective potential $U$ (if the noise is small and the system spends most time near attractors). This links to the concept of an epigenetic landscape used in cell differentiation models ￼, which has been applied to disease states as well ￼ ￼. We can leverage such methods to estimate an energy landscape for CMSS given data, identifying the “width” and “depth” of the disease attractor compared to healthy attractor (as has been done in cancer vs normal cell attractors ￼).

Using these approaches, we will quantify statements like: “The disease state is a deeper/wider attractor basin than the healthy state under parameter set X”, or “As parameter $\Lambda$ increases, the healthy attractor shallows and eventually a bifurcation occurs introducing a new deeper attractor corresponding to chronic illness.” Indeed, prior work in complex diseases suggests that pathological networks can create robust attractors that trap the system ￼. Our framework aims to mathematically capture this trapping via nonlinear dynamics.

Lyapunov Exponents and Chaos

While much of our discussion assumes fixed-point or cyclic attractors, it is possible the system exhibits chaotic dynamics in certain regimes (especially with delays and nonlinear feedbacks). We will compute Lyapunov exponents to assess chaos. A positive largest Lyapunov exponent would indicate sensitive dependence on initial conditions – potentially relevant to the “unpredictable” nature of symptom fluctuations. Some researchers have applied chaos theory to physiological and psychological disorders ￼, finding that instability patterns resembling low-dimensional chaos can occur in disease processes ￼. If our model (for some parameter sets) yields chaotic trajectories, we would then interpret the chronic syndrome as a chaotic dynamical state (distinct from a stable equilibrium or limit cycle). This could correspond to erratic symptom variation even without external perturbations.

However, even if chaos is present, the long-term statistical behavior might still be summarized by an attractor (a strange attractor in state space). We would analyze the fractal dimensions of any chaotic attractor and the Lyapunov spectrum to understand the structure of variability. We might also examine if chaos transitions to order via parameter changes (e.g. via period-doubling route ￼ or intermittency) – such transitions could manifest as changes in the pattern of symptom oscillations (from irregular to periodic or vice versa). For instance, in one model of Parkinsonian tremor, increased delay feedback led to a sequence of period-doubling bifurcations culminating in chaos ￼; analogously, perhaps increasing a feedback gain in our model could move fatigue oscillations from periodic crashes to chaotic unpredictable crashes.

In summary, the stability analysis portion of this proposal will thoroughly investigate how the attractor landscape of the system depends on key parameters. We will identify conditions for monostability (robust single healthy state), bistability (healthy vs diseased states coexisting), oscillatory instabilities (limit cycles, possibly quasiperiodic or chaotic attractors), and attractor transitions. This forms the theoretical core of the “Linked Instability Theorem” – describing how linked systems (genetic, autonomic, structural, etc.) can lead to emergent instabilities that underpin chronic multi-system disorders.

Information-Theoretic and Thermodynamic Measures

To enrich our analysis, we incorporate concepts from information theory and thermodynamics. These measures help quantify the system’s organization, signaling, and departure from equilibrium. They provide a bridge between abstract dynamics and physiologically meaningful metrics (like “loss of complexity” in disease ￼ ￼). Below we outline how mutual information, entropy production, coherence, and complexity will be defined in our framework:

Mutual Information and Cross-Talk

Mutual information (MI) measures the amount of information one variable carries about another. In a multi-variable dynamical system, a high mutual information between two components indicates they are strongly coupled or share a common influence. Formally, for two state variables $x_i(t)$ and $x_j(t)$ with joint probability distribution $P(x_i, x_j)$ (considering their values over time or across an ensemble), the mutual information is:

$$
I(x_i; x_j) = \iint P(x_i, x_j) \log \frac{P(x_i, x_j)}{P(x_i),P(x_j)},dx_i,dx_j,.
$$

In practice, we can compute MI between time-series (perhaps after some appropriate discretization or binning) for different pairs of subsystems. This helps identify which subsystems are most inter-dependent. For example, one might find high $I(A; I)$ between autonomic tone and immune activation during flares, indicating tight coupling (consistent with an inflammatory reflex loop). On the other hand, if a subsystem becomes decoupled (e.g. circadian rhythm decoupled from sleep cycle in ME/CFS), the mutual information between those signals would drop.

We will use mutual information to quantify coherence vs independence of subsystems in health and disease. A healthy system may have an optimal level of interconnection – some mutual information, but not total locking. If mutual information is too high, subsystems may be excessively synchronized (losing independent variability), whereas if too low, the systems may lack coordination. CMSS might involve pathological mutual information patterns: for instance, an overly high MI between pain signals and autonomic responses could mean the patient’s pain and heart rate are locked in a vicious positive feedback (pain → panic → more pain). By contrast, low MI between HPA axis and immune system could mean the normal cortisol-mediated regulation of inflammation is lost, so they behave independently when they should not.

Information-theoretically, we can also examine transfer entropy (a directional version of MI for time series) to see the direction of influence (e.g. inflammation driving autonomic changes vs autonomic driving inflammation). This will complement the structural $\Xi$ parameters by providing data-driven validation: if our model says $\Xi_{I,A}$ is large, we should see a high information flow from $I$ to $A$ in simulated (or real) data.

Entropy Production and Thermodynamics

Living systems are far from thermodynamic equilibrium, constantly dissipating energy to maintain order ￼. Entropy production in our model can be analyzed by treating the system as a non-equilibrium thermodynamic system. Each process (reaction, feedback loop) can be associated with entropy change. One approach is to derive a thermodynamic formalism for the model: identify generalized fluxes and forces for each exchange and use the second law of thermodynamics ($dS_{\text{total}} = dS_{\text{system}} + dS_{\text{env}} \ge 0$). The entropy production rate $\sigma_{\text{prod}}$ is the rate of entropy increase in the system+environment due to irreversible processes.

In a coarse-grained way, we can track an approximate entropy for the physiological state. For example, Bienertová-Vašků et al. (2016) introduced a concept of Stress Entropic Load (SEL), distinguishing entropy production above basal metabolism as a measure of stress on the organism ￼. Similarly, we might define baseline entropy production for homeostasis vs excess entropy production when the system is in a diseased flare. Chronic inflammation, for instance, is an energy-consuming, entropy-producing process (e.g. high cytokine turnover, acute phase reactions). We anticipate that a CMSS state might correlate with higher entropy production than a healthy resting state, reflecting the inefficiency and internal friction of a dysregulated system. This aligns with the idea that chronic illness often involves chronic activation of processes that should be occasional (thereby consuming resources and generating entropy continuously).

We will attempt to calculate or estimate:
	•	The total entropy production in steady-state (using a formula from non-equilibrium thermodynamics if we model each feedback loop’s energetics). For instance, if our model includes metabolic rate $E(t)$, we can link it to heat dissipation. One could use $S_{\text{prod}} = \int_0^T \frac{\dot{Q}(t)}{T_{body}} dt$ over a period, where $\dot{Q}$ is heat dissipated and $T_{body}$ body temperature in Kelvin, as a proxy for entropy generated by metabolism. Chronic high inflammation will increase $\dot{Q}$ (fever, etc.) thus entropy.
	•	The entropy flow between subsystems. If we consider an open system perspective, one subsystem’s entropy can be exported to another. For example, the autonomic nervous system might act to export entropy from the cardiovascular subsystem by increasing heart rate variability (randomness) to buffer blood pressure fluctuations. If such balancing is impaired, entropy accumulates in one part (meaning more disorder in that subsystem).
	•	Use the Fluctuation-Dissipation Theorem concept: systems near equilibrium have a relation between the variance (fluctuations) and dissipation (entropy production). In our model, if we linearize near an attractor, we could check if elevated fluctuations (due to high σ noise) correlate with more dissipation required to maintain stability.

We will also consider Maximum Entropy Production Principle hypotheses ￼ in context of physiology: some theorize organisms might operate near maximum entropy production consistent with constraints, as a way to maintain homeostasis. Alternatively, disease could either increase entropy production (system in overdrive) or decrease it (system in anergy or stagnation). By measuring entropy production in our simulations under different parameter regimes, we can see which applies. For example, an ME/CFS crash might be a low entropy production state (patient appears to have low metabolic output, “energy conservation mode”), whereas a flare (like post-exertional malaise) might be high entropy production (inflammation, stress response on full blast). Our model can reproduce these by appropriate parameter shifts and we can quantitatively compare the entropy changes.

In summary, entropy measures will give a quantitative handle on how far the system is from equilibrium and how much “stress” it’s under in the thermodynamic sense ￼ ￼. This can complement conventional stability analysis by showing, for instance, that even if an attractor exists, it might require a high entropy cost to stay there (hence not sustainable long term).

Coherence and Rhythmic Stability

Coherence in this context refers to the degree of synchronous, orderly behavior among components, especially in oscillatory dynamics. We will evaluate coherence in two senses:
	1.	Spectral coherence between time-series (are two signals oscillating with a consistent phase relationship at a certain frequency?), and
	2.	Coherence resonance (the phenomenon where noise induces the system to oscillate more regularly at a preferred frequency).

For spectral coherence, imagine we have a slow oscillation (~0.1 Hz) in blood pressure due to baroreflex and a similar oscillation in heart rate (this is analogous to Mayer waves and RSA in autonomic function). In a healthy system, these might be tightly coupled (high coherence) indicating robust feedback. In dysautonomia (like POTS), that coherence might break down. We can compute coherence functions $C_{ij}(f)$ between pairs of signals to see which subsystems remain aligned. A high coherence at frequency $f$ means subsystem $i$ and $j$ are locked in an oscillation at $f$. For example, high coherence between immune and autonomic oscillations might indicate an underlying cytokine circadian rhythm driving symptoms on a 24h cycle.

Coherence can also be examined globally: are all subsystems cycling together or are they fragmented? Too much global coherence could mean the system’s variability is dominated by a single mode (potentially a sign of reduced complexity), while too little could mean loss of coordination (like an orchestra out of sync).

We will also investigate phase coherence in any limit cycles the model produces. Using phase reduction methods, we can study if the oscillatory dynamics have a well-defined phase and whether noise causes phase slips. This could connect to phenomena like why certain triggers (e.g. sleep deprivation) cause a phase shift in symptom cycles.

On the other hand, coherence resonance might be relevant: sometimes moderate noise actually regularizes the oscillations of a system near a Hopf bifurcation. If a patient’s symptoms have a somewhat periodic flare pattern, it could be noise-driven coherence resonance. By simulating at different noise levels $\sigma$, we can check for maximal regularity of oscillation (e.g. measure the variance of inter-flare intervals as a function of σ).

In summary, coherence measures will tell us about the synchrony and regularity of multi-system oscillations. In practical terms, we might find that a healthy system exhibits multi-frequency behavior (each subsystem doing its thing but coordinated when needed), whereas a pathological system might show either (a) a dominant pathological oscillation (e.g. a sympathetic overactivity oscillation seen in heart rate and blood pressure and even immune markers all in unison) or (b) a breakdown of normal coherence (e.g. heart rate and blood pressure no longer align, reflecting autonomic control loss). Both scenarios are possible in different conditions, and our framework can capture either depending on parameter regimes.

Complexity and Variability (Loss of Complexity Hypothesis)

One hallmark observation in physiology is that healthy systems exhibit complex variability, whereas aging or disease often leads to either highly periodic or very random behavior – both of which are “less complex” than healthy variability ￼. This is known as the loss of complexity hypothesis ￼ ￼. We will quantify complexity in our model’s outputs using metrics such as:
	•	Approximate entropy (ApEn) or sample entropy: to gauge the regularity/unpredictability of time series. Lower entropy (more predictability) can mean either very periodic or very static behavior, indicating low complexity. Very high entropy means completely random (uncorrelated noise), also low complexity in a sense (no structure). Healthy complexity lies in between, with long-range correlations (fractal-like).
	•	Fractal measures (Hurst exponent, DFA): to see if signals have multi-scale variability. Healthy heart rate, for example, has fractal scaling close to $1/f$ noise; heart failure patients often show either too much randomness or too periodic patterns, deviating from fractal scaling ￼ ￼.
	•	Multivariate complexity (e.g. multivariate sample entropy): to capture joint complexity across subsystems, possibly reflecting integrated but flexible dynamics.

Our model can reproduce loss of complexity when it settles into an attractor with either overly periodic dynamics or overly damped dynamics. For instance, if the system falls into a rigid limit cycle (all variables oscillating in a fixed pattern), the complexity is low (just one frequency). If it falls into a fixed point with only noise perturbations, and if noise is white, then long-range correlations are lost (only short-term randomness). Both could correspond to patient states: some fibromyalgia patients might have very regular pain oscillations (low complexity), others might have erratic pain but no structure (also low complexity). The optimal complexity is seen in a healthy adaptable system: a mix of frequencies and fluctuations across scales ￼.

We aim to show that as parameters move into the pathological regime, complexity measures drop. This can be tied to the concept of approaching a critical point (critical slowing down often reduces high-frequency variability). Interestingly, complexity loss can be both a consequence and a warning sign of bifurcation. We might leverage this: by monitoring model complexity, one could foresee a pending transition. In clinical context, this would mean measuring a patient’s physiological signals for loss of complexity as an early-warning of crash or deterioration ￼.

Additionally, we will explore Lyapunov spectrum complexity: e.g. the Kaplan-Yorke dimension if chaos is present. A healthy system might actually have a strange attractor with a modest dimensionality (indicating a richly interactive system), whereas a diseased one might collapse to a limit cycle (dimension ~1) or become nearly periodic. Prior work by Goldberger et al. demonstrated that disease and aging reduce physiologic complexity, thereby diminishing adaptive capacity ￼. Our model will incorporate this notion and attempt to reproduce such findings quantitatively.

In summary, by evaluating these information-theoretic and complexity metrics, we add a layer of quantitative phenotype to the model. Instead of just saying “the system is unstable or stable,” we can say “the system in condition X has mutual information Y between these subsystems, entropy production Z, and complexity C – all deviating from healthy values.” This provides potential biomarkers (in silico) and can guide what to measure in patients to validate the model (for instance, measure heart rate complexity or cytokine network entropy in patients vs controls ￼).

Extensions: Stochasticity, Network Modeling, Control, and Information Geometry

Thus far, we described the core ODE/DDE framework and some analytical approaches. We now detail several extensions to deepen the model:

Stochastic Differential Equations (SDE) Extension

In reality, biological systems are noisy, and that noise can qualitatively change dynamics. To capture this, we extend our ODEs to stochastic differential equations. A generic form for our system including noise is:

$$
d\mathbf{x} = F(\mathbf{x}(t), \mathbf{x}(t-\tau_\ell);; \boldsymbol{\Theta}),dt ;+; B(\mathbf{x}(t);; \boldsymbol{\Theta}),d\mathbf{W}_t,,
$$

where $B(\mathbf{x})$ is a diffusion matrix shaping how noise $d\mathbf{W}_t$ (a vector of Wiener increments) enters each equation. For simplicity, one might start with $B$ constant or diagonal ($B = \text{diag}(\sigma_1,\dots,\sigma_n)$ so that $dx_i = F_i,dt + \sigma_i,dW_i$), meaning independent noise in each variable. Eventually, $B$ could be made state-dependent to reflect that noise amplitude might increase when a system is more active (e.g. more inflammation → more variance in inflammatory mediators).

Stochastic simulation will allow us to study probabilistic outcomes, not just deterministic trajectories. We will run ensembles of simulations to see distributions of states over time. In a chronic illness context, this is relevant because not every patient with the same parameters has identical experiences – random events (like infections or stressors) can push individuals differently.

We will examine phenomena such as:
	•	Noise-induced transitions: If the system is bistable (healthy and disease attractors), noise can cause random jumps between attractors. By calculating the mean first passage time for noise-driven escape from one basin to the other, we can quantify how often a patient might spontaneously go into remission or relapse. This is analogous to Kramers’ escape rate over a potential barrier: $T_{\text{escape}} \sim \exp(\Delta U / D)$, where $\Delta U$ is barrier height and $D$ noise intensity. In our model, $\Delta U$ might correspond to the difference in Lyapunov functions between states, and $D$ relates to $\sigma$. We expect that for severe cases (deep pathological basin), $T_{\text{escape}}$ is essentially infinite without intervention (thus chronic illness persists), whereas mild cases with shallower basins might occasionally remit.
	•	Stationary distribution and detailed balance: If we ignore time-delays for a moment, the SDE defines a Fokker-Planck equation for $P(\mathbf{x},t)$. We will seek the stationary distribution $P_{\infty}(\mathbf{x})$. If detailed balance approximately holds (like in a nearly gradient system with noise), $P_{\infty} \propto e^{-U(\mathbf{x})}$ and we can interpret $U(\mathbf{x})$ as a potential. Even if not, $P_{\infty}$ gives us the probability weight of being near each attractor. This can be compared qualitatively to epidemiological observations (e.g. what fraction of time patients spend in flare vs relative calm). It also allows computation of entropy of the distribution (how spread out states are).
	•	Entropy production rate: In a stochastic system, one can compute entropy production from the probability flux if the process is out of equilibrium. Using methods from stochastic thermodynamics, we can calculate the entropy produced in the environment due to irreversible transitions. This complements our earlier thermodynamic analysis by providing a more rigorous calculation from the SDE (e.g. using $dS_{\text{prod}} = \int (D^{-1}( \text{drift}\cdot \text{flux})) dt$ in some formulations). A high entropy production in steady-state is a signature of far-from-equilibrium dynamics, which likely correlates with the “stress” state of the system ￼.

Mathematically, introducing stochasticity means we will often deal with expectations and variances rather than single trajectories. We will apply tools like moment closure (to derive ODEs for means and covariances) or simulate directly using Euler-Maruyama or more sophisticated SDE solvers. The analysis of bifurcations can be extended to stochastic bifurcations (e.g. changes in stationary distribution topology or switching rates).

Including noise also lets us consider random delays or random parameters (to model heterogeneity). Each individual might have slightly different $\boldsymbol{\Theta}$; we can simulate a population by sampling parameter distributions and see common patterns.

In summary, the SDE extension ensures the model is more realistic and captures the unpredictable fluctuations and rare events seen in CMSS (like sudden crashes). It also opens the door to applying the rich theory of stochastic processes to understand resilience and failure in these systems.

Network Theory: Graph Representation of Interactions

The GASLIT-AF framework can be represented as a network of interacting components. Each state variable $x_i$ is a node in a directed graph, and an influence of $x_j$ on $x_i$ (a nonzero partial derivative $\partial F_i/\partial x_j$) is represented as a directed edge from node $j$ to node $i$, weighted by the coupling strength (which could be one of the $\Xi_{ij}$ parameters or a function thereof). This network view aligns with the field of network medicine, which posits that diseases are disturbances of network topology and dynamics rather than isolated defects ￼. In fact, Barabási et al. emphasize that a disease phenotype “reflects perturbations of the complex intracellular and intercellular network that links tissue and organ systems” ￼, which is exactly the scenario for CMSS.

Key steps and benefits of the network approach:
	•	Network topology analysis: We will formally define the adjacency matrix $A = [a_{ij}]$ where $a_{ij} = 1$ if $x_j$ directly influences $x_i$ (and zero otherwise). Weighted versions will use the Jacobian or influence strengths. By studying $A$, we can identify motifs like feedback loops (e.g. $i \to j \to i$ loops), feedforward chains, or bottleneck nodes. For example, if one node has many outgoing edges (high out-degree), it’s an “influencer” in the system; if one has many incoming (high in-degree), it’s an integrator of multiple signals. We suspect nodes like autonomic output or central neuroendocrine regulators might be high-degree hubs in CMSS networks (consistent with the autonomic nervous system’s wide-reaching effects).
	•	Community detection and modules: Perhaps the network can be partitioned into modules (e.g. an immune-inflammatory module, a neuroendocrine module, a cardiovascular-autonomic module, etc.) that correspond to organ systems. Edges between modules are cross-system interactions. In health, modules might be semi-autonomous (weak inter-module connections), but in disease, some inter-module connections might strengthen (blurring boundaries, e.g. chronic inflammation strongly impacting nervous system = neuroinflammation). We can apply graph algorithms to identify these modules and see how they shift with parameter changes.
	•	Dynamical network analysis: Using techniques like network control theory or spectral graph analysis, we can relate network structure to dynamics. For example, the eigenvalues of the Jacobian are related to the eigenvalues of adjacency in linear approximations. A highly connected positive feedback loop corresponds to a subgraph whose adjacency has an eigenvalue that can exceed 1 (discrete-time) or whose real part can exceed 0 (continuous-time), indicating possible instability. Thus, we can pinpoint which sub-network (set of interactions) is responsible for a given instability. This is helpful: if, say, a Hopf bifurcation comes from the interactions of nodes {A, I, N} (autonomic, immune, neural) strongly mutually coupled, then therapy might target breaking that coupling.
	•	Network robustness and attack simulations: We can simulate “attacks” or failures of nodes/edges (this is like doing in silico experiments of cutting an interaction). For instance, remove or weaken an edge that corresponds to an intervention (e.g. a drug that blocks a cytokine – effectively setting $\Xi_{\text{cytokine} \to \text{neuron}} \approx 0$). We then see if the network’s dynamics move back to stability. Network theory provides measures like node centrality (which node’s removal has biggest effect on connectivity). We will connect this with control (below) to identify optimal intervention targets – often the most central or bottleneck nodes in pathological feedback loops.

Furthermore, we can overlay multi-layer networks if needed: one layer for genetic interactions, one for protein signaling, one for organ-level physiology. But that may be beyond our current scope; we mostly focus on a phenomenological network at the organ/system level.

The network perspective also aligns with data-driven approaches: e.g. building a network from patient data correlations and see if it matches our model’s network structure ￼. As an example, a recent network medicine analysis of ME/CFS genetics looked for network modules enriched in patient data ￼. Our model could incorporate those findings by ensuring the corresponding interactions are present in $A$. Conversely, our model could predict that a certain obscure interaction is critical; experimental network studies could then verify if patients exhibit that connection (say, an unexpected link between connective tissue and immune signaling mediated by damage molecules).

Control Theory and Intervention Modeling

One ultimate aim of a systems biology model is to test interventions – what can push the system from illness back to health? To approach this, we utilize control theory concepts. We treat certain variables or parameters as control inputs $u(t)$. For example, an external control could be a medication that directly lowers inflammation ($u_I(t)$ that enters $\dot{I}$ with a negative sign), or a pacemaker-like device that influences heart rate variability ($u_A(t)$), or even behavioral interventions like exercise as a forcing function on muscle metabolism variables.

Key control theory aspects:
	•	Controllability: We will examine if and how the system can be driven from one state to another via admissible controls. Linearizing about the healthy state and about the diseased state, we can check the controllability matrices. If the linearized system $(A, B)$ (where $A$ is Jacobian, $B$ is input matrix) is controllable, then in theory a time-varying control can transfer the state. Nonlinear controllability (using Lie brackets) can also be considered, but practically, linear analysis around attractors is a start.
	•	Stabilization: We might design a feedback controller $u(t) = K \cdot (\mathbf{x}_{target} - \mathbf{x}(t))$ to stabilize the healthy state (making it more attractive). For instance, biofeedback therapies could be seen as implementing a control law that tries to counteract deviations (like heart rate biofeedback trains patients to downregulate sympathetic surges, effectively adding negative feedback). We can model such strategies and simulate their effect.
	•	Optimal control: Using techniques like Pontryagin’s maximum principle or dynamic programming, we can attempt to find an optimal intervention strategy to minimize “symptom burden” or to minimize time to recovery. The cost function might penalize distance from healthy equilibrium and also penalize drug dosage (to find a balance). This yields insights such as: Is it better to give a strong short pulse intervention or a weaker long-duration one? Our earlier insight from the IDO model was that escape requires an exogenous perturbation ￼ – control theory can quantify how big that perturbation must be and of what form (impulsive, oscillatory, continuous, etc.).
	•	Delay and robustness: Because our system has delays, we will consider control in delay systems (which is more complex, but some theory exists). We will also test robustness of controls: a controller designed on the model should still work if parameters are slightly different (patient heterogeneity). This leads to adaptive control ideas – can the controller adjust if it detects the patient’s response is different?

A practical example: Suppose our model identifies a high $\Lambda_{N}$ (neural sensitization feedback) as a culprit for a fibromyalgia pain attractor. A control strategy might be a neuromodulation that effectively reduces $\Lambda_N$ over time (e.g. graded exercise therapy might slowly raise the pain threshold $\Theta_{\text{pain}}$, or medications like SNRIs might increase damping $\gamma_N$ on pain signals). We can simulate these as time-dependent changes and see if the system transitions to the healthy state. If the model suggests it does, that provides a rationale to pursue that therapy and a way to test dosing schedules in silico.

Another example: POTS (Postural Orthostatic Tachycardia Syndrome) might be represented by an autonomic loop that has become too sensitive (low $\Theta$ for baroreceptor firing) and high gain (high $\Lambda$ in heart rate response). A control (like a pacemaker or ivabradine drug) might effectively cap $\Omega$ or $\Lambda$ to prevent runaway heart rate upon standing. We’d model the act of standing as a perturbation and see how control mitigates the tachycardia.

Validation strategy using control: We can simulate historical interventions and see if our model replicates their effects. For instance, saline infusions often help POTS temporarily – in the model, adding blood volume could be a parameter shift that stabilizes short-term. If our model doesn’t reflect that, it may be missing a volume-related variable.

In sum, incorporating control theory turns our descriptive model into a prescriptive tool: it allows asking “what if we do X?” in a rigorous way. The nonlinear, multi-delay nature of the system will require careful design of control strategies, possibly using computational search (since analytic solutions for optimal control might be infeasible). But even brute-force search in simulation can yield candidates for effective interventions or combinations thereof.

Information Geometry of the Parameter Manifold

Complex dynamical models often have many parameters, and not all are independent in effect. Information geometry provides a way to study the structure of the model’s parameter space by endowing it with a Riemannian metric based on the Fisher Information Matrix (FIM). Each parameter vector $\boldsymbol{\Theta}$ corresponds to a probability distribution of model output (or a deterministic mapping in the case of no noise, but we can consider an observational noise to get a distribution). The Fisher information metric is:

$$
g_{ij}(\boldsymbol{\Theta}) = \mathbb{E}!\Big[ \frac{\partial \ln L(\boldsymbol{\Theta}|data)}{\partial \Theta_i},\frac{\partial \ln L}{\partial \Theta_j} \Big],,
$$

which intuitively measures how sensitively the model’s output changes with parameter changes. This defines a parameter manifold where distance reflects distinguishability of parameter sets given their outputs ￼.

Why is this useful? It helps us identify sloppy vs stiff directions in parameter space ￼. Often, models like this have some combinations of parameters that are very well determined by system behavior and others that hardly matter (the system might have redundancies). Sethna et al. have shown that many biological models are sloppy, meaning eigenvalues of the FIM span many orders of magnitude ￼. A few eigenvectors (parameter combinations) are “stiff” (important) and many are “sloppy” (unimportant to output) ￼. We will apply this analysis to GASLIT-AF by computing or approximating the FIM around different regimes (healthy or diseased).

If we find, for example, that a particular combination of $\Xi$ and $\Lambda$ is what really matters for outcome (a stiff direction) and others like slight variations in $\gamma$ are sloppy, that informs us where to focus experimentally. It also might reflect evolution: biological systems often evolve to be robust to certain parameter variations (sloppy parameters) but sensitive to a few key ones ￼, possibly those under tight homeostatic control.

Additionally, by analyzing the curvature of the parameter manifold, we can detect degeneracies (flat directions correspond to symmetries or non-identifiabilities in the model) ￼. For instance, maybe increasing $\Lambda$ and $\gamma$ together produces no net change in outcome (one makes feedback stronger, the other damps it equivalently). Such insights can simplify the model by suggesting a reduced parameter combination (like a ratio $\Lambda/\gamma$ is what actually matters, often seen in Michaelis-Menten style dynamics where only a combination is identifiable).

We can visualize the model manifold if we reduce it to 2D or 3D via methods like principal component analysis on the FIM or via Manifold learning of model outputs ￼. If the manifold has a hyperribbon shape (very thin in many dimensions, thick in a couple) ￼, that confirms sloppiness. We can then propose an “effective model” with fewer parameters capturing the key stiff directions (this is like finding an emergent simpler theory ￼). This could be important for making the model practical – perhaps a simpler model captures the essence of GASLIT-AF without needing every parameter to be tuned.

Finally, information geometry can inform experimental design: the Fisher information tells us which experiments (perturbations or measurements) would most improve our knowledge of parameters ￼. For example, if two parameters are unidentifiable from passive observation, we might simulate an intervention that separates their effects. In a proposal, we’d outline such approaches to refine parameter estimates using information-guided experiments (like challenge tests to patients and measure responses, chosen to maximally inform the model about their internal parameter state).

In conclusion, applying information geometry ensures our model is not just a wild parametrization, but one where we understand the meaningful degrees of freedom and can fit it to data in a principled way. It also ties into the concept of model robustness: a well-functioning system might lie in a flat valley of the parameter manifold (robust to perturbations), whereas an edge-of-instability system might be at a curved cliff (small param changes cause big outcome changes). We will investigate if CMSS patients indeed correspond to being near such “curved” regions, which would align with the idea of them being sensitive to small triggers (param fluctuations leading to big symptom changes).

Biological Pathway Integration: Examples (KLK15 Variant and IDO Trap)

To ensure the GASLIT-AF model has biological grounding, we map specific known pathways or genetic factors to our model parameters. Here we illustrate with two examples – the KLK15 genetic variant associated with hEDS, and the IDO metabolic trap hypothesis in ME/CFS – how these real findings modulate the model.

1. Kallikrein-15 (KLK15) and Connective Tissue-Initiated Feedback: Hypermobile EDS is a hereditary connective tissue disorder with an unknown genetic basis until recently. A study by the Norris Lab (2024) discovered a specific variant in the KLK15 gene that segregated with hEDS in multiple families ￼. KLK15 encodes kallikrein-15, a protease that likely affects extracellular matrix or signaling peptides. Mice engineered with the KLK15 variant showed connective tissue problems like weaker tendons and defective heart valves, mirroring hEDS features ￼. In our model, we introduce a state variable $S(t)$ for structural integrity (connective tissue stability). The KLK15 variant can be modeled as a change in parameters related to $S$. For instance:
	•	It could lower the baseline value of $S$ or its production rate, meaning tissues are inherently weaker (like having a lower equilibrium collagen integrity). This might be a parameter $\Lambda_S$ (if $S$ is maintained by some production term) being smaller.
	•	It could increase the sensitivity of damage signals. Weak connective tissue means minor stresses lead to microdamage and release of alarmins/bradykinin. In the model, this could be a lowered threshold $\Theta_{S\to I}$ for activation of inflammation from structural strain. Essentially, $\Theta$ for the link “Structural→Inflammation” is reduced, so even normal activity triggers an outsized inflammatory response (allodynia).
	•	It might also increase coupling $\Xi_{S\to N}$ (Structural to Neural pain) or $\Xi_{S\to A}$ (Structural to Autonomic). For example, if bradykinin (produced in excess due to KLK15 variant) activates pain fibers and mast cells, we effectively have a stronger link from structural microdamage to pain and allergic responses.

In summary, the KLK15 variant tips the “Structural” part of the system toward instability: tissues under stress generate high feedback into other systems. With such a variant, the system might be closer to a threshold where a little injury causes a big systemic reaction (pain flare, POTS episode from pain, etc.). We can simulate two scenarios: with normal KLK15 vs mutant KLK15 parameters. The mutant scenario should show reduced $S$ and possibly an existing low-level inflammation/pain even without external triggers (due to spontaneous microinjuries). This aligns with patients who have daily pain/inflammation even without obvious injury. The model can then test if strengthening $S$ (conceptually, therapies to improve connective tissue, e.g. certain supplements or drugs that increase collagen cross-linking) would raise those thresholds and dampen the whole cascade.

2. IDO Metabolic Trap and Immune-Metabolic Bistability: The indoleamine 2,3-dioxygenase (IDO) pathway controls tryptophan metabolism to kynurenine and is involved in immune tolerance. A hypothesis by Phair et al. proposes that some ME/CFS patients have a genetic setup (common non-functional variants of IDO2 enzyme) that can lead to a metabolic trap: once an immune trigger drives IDO1 high, the feedback on tryptophan and IDO2 inability causes the system to get stuck in a pathological low-tryptophan, high-kynurenine state ￼. This is essentially a bistable switch in metabolism. They developed a mathematical model showing normal and pathological steady states for tryptophan metabolism, and that exiting the pathological state required an external perturbation ￼.

We incorporate this by adding a simplified tryptophan–kynurenine subsystem in our model, or by abstracting it as part of the immune/inflammation dynamics. Let $T(t)$ be tryptophan availability and $K(t)$ be a proxy for kynurenine or downstream consequences (like NAD+ levels). The IDO trap can be encoded as a positive feedback: high $K$ (from high IDO1 activity) leads to an enzyme inhibition that keeps $T$ low, which in turn fails to feed back and turn off inflammation (since tryptophan is needed for neurochemicals like serotonin, etc.). We might not explicitly simulate all biochemistry, but the key effect is a bistability in the immune-metabolic module. Parameter-wise:
	•	A nonfunctional IDO2 means the alternate pathway to clear tryptophan is absent. In the model, this could mean a parameter $\gamma_T$ (tryptophan decay rate via IDO2 path) is ~0. So tryptophan clearance relies solely on IDO1 which has substrate inhibition at high levels.
	•	Substrate inhibition of IDO1 is a nonlinear effect – we would include a term like $\text{Rate}{IDO1} = V{max} \frac{T}{T + K_m + \alpha T^2}$ (just conceptually) which can cause two stable points. This introduces a nonlinear term in $F(\mathbf{x})$ for that subsystem.
	•	The result is two steady states for $T$: one with normal $T$ (healthy, IDO1 moderate) and one with low $T$ (pathological, IDO1 saturated and inhibited). Because $T$ also influences, say, neural function (low tryptophan -> low serotonin -> fatigue/depression) and immune regulation (low tryptophan can keep T cells activated), falling into this trap means the whole system shifts: immune system might remain in an active/inflammatory mode thinking there’s ongoing infection (since tryptophan deprivation is a defense mechanism signal).

By plugging this into the larger model, we see an example of a molecular pathway creating a system-wide attractor. The IDO trap essentially could instantiate the “chronic immune activation” attractor. If a patient has that predisposing mutation (so parameters set accordingly), then after an infection that triggers interferon (which raises IDO1), they could slide into the pathological attractor and not easily return. Our simulation would show that with IDO2 = 0 and high initial immune stimulus, the inflammation $I(t)$ and metabolism $T(t)$ settle into an abnormal equilibrium (with low $T$ sustaining higher $I$ perhaps). Conversely, if IDO2 is functional (a parameter difference), the system only has one equilibrium or can recover as $I$ drops. This matches the hypothesis that common genetics (IDO2 variants found in many people) could explain why a subset get “stuck” after an immune trigger ￼.

We will validate the inclusion of this pathway by comparing model predictions to known observations: e.g., measure blood tryptophan and kynurenine in patients (which studies have done ￼) – many ME/CFS patients indeed show altered kynurenine/tryptophan levels ￼. If our model’s $T$ and $K$ behave similarly, that’s support.

Feedback into the Model: Both KLK15 and IDO examples show how genetics/environment set parameters that predispose to instability. We can incorporate many other pathways similarly: e.g. autoantibodies in POTS affecting $\Xi_{ANS}$, or mast cell activation lowering threshold $\Theta$ for allergic inflammation. The modular design of our ODEs means each known factor either tweaks an existing parameter or adds a new variable/interaction. Over time, we expect to refine the model with more such mappings as research uncovers them.

Model Validation and Implementation Strategy

Finally, we outline how this GASLIT-AF model can be validated, calibrated, and used experimentally:
	•	Data Sources for Calibration: We will utilize clinical data from CMSS patients – for example, time-series of heart rate, blood pressure, activity levels (wearables), cytokine measurements, etc. These can be used to fit certain subsystems (e.g. match model’s autonomic oscillation frequency to observed heart rate oscillation). We will also use perturbation studies when available (tilt-table tests for POTS, exercise challenges for ME/CFS) to see if the model reproduces the responses. If possible, we might use high-dimensional data like gene expression or proteomics to inform network structure (as in building a gene network and seeing its attractors ￼, then linking to our coarse-grained variables).
	•	Parameter Estimation: The large parameter set will be constrained by literature values for known kinetics when possible (e.g. half-life of a cytokine gives $\gamma_I$, reaction rates for IDO give an idea for its parameters, etc.). For unknown parameters, we will use optimization techniques to fit model outputs to patient data. Approaches include simulated annealing, evolutionary algorithms, or Bayesian MCMC to sample the posterior of parameters given data. Information geometry insights (sloppy vs stiff directions) will help regularize this process by telling us which parameter combinations can be determined by a given dataset ￼. We might also fit simpler sub-models module by module (e.g. fit an inflammation module separately with data from an acute infection, fit an autonomic module with data from tilt test) then combine them.
	•	Validation of Predictions: After calibration, we will test the model’s predictive power. For instance, if the model suggests that raising a certain parameter (simulating a drug) leads to recovery, and that drug is known clinically to help, that’s a retrospective validation. More powerfully, the model might predict novel interventions or identify biomarkers; those can be prospectively tested. One could also validate the existence of predicted attractor states by searching for them in data – e.g. cluster analysis on patient data to see if they group into states that correspond to model’s attractors. If the model says there’s an intermediate semi-stable state (perhaps a compensation phase), we can see if any patients appear to be in that intermediate state.
	•	Robustness Analysis: We will perform in silico robustness tests: vary parameters within ranges and add noise to see if qualitative behaviors persist. A useful measure will be to compute the probability of the system developing chronic illness given random perturbations, under different parameter regimes. Ideally, healthy regime has extremely low probability of tipping into illness (like most people don’t get ME/CFS after infection), whereas a predisposed regime has a significantly higher probability. We can try to match those probabilities to epidemiological data (e.g. ~10% of people after severe COVID get Long COVID, which might correspond to certain param distribution).
	•	Comparing Syndromes: Since CMSS includes multiple conditions, our model should be flexible to reproduce features of each by tweaking parameters. For example, fibromyalgia vs ME/CFS might differ in which variables dominate (pain vs fatigue focus). We will show that by changing parameter emphases, the model can shift phenotype. If data permits, we can attempt to fit one parameter set to a fibromyalgia cohort and another to an ME/CFS cohort, and see what differences emerge (perhaps fibro has higher neural gain $\Lambda_N$, ME/CFS higher metabolic trap effect, etc.). Finding such differences would validate that the model can distinguish subtypes while still being under one framework.
	•	Use of Bifurcation as Biomarker: An intriguing validation is the concept of critical slowing down as one approaches a bifurcation. This manifests as increased variance and autocorrelation in time-series. We could examine if patients, before a crash or relapse, show increased variability in certain measures – this is a known early warning signal of bifurcation in dynamical systems. If our model shows it and patients do too, it’s a sign the model’s dynamics align with reality. Recent studies in diseases have looked for such warning signals ￼.
	•	Interdisciplinary Collaboration: This proposal will be executed with input from clinicians and experimentalists studying these syndromes. The model’s assumptions (like which variables to include, what functional forms make sense) will be informed by up-to-date biomedical knowledge, and in turn the model may suggest experiments (for example, measuring both kalikrein levels and tryptophan metabolism in the same patients to see if those subsystems indeed interact as model suggests). The framework is testable: each coupling or feedback could potentially be checked with interventions (block that pathway and see if the patient improves in line with model prediction).
	•	Technical Implementation: We will implement the model in a simulation environment (such as Python with libraries for delay differential equations and SDEs, or MATLAB/Simulink for ease of control design). We will also use analytical tools (e.g. XPPAUT for bifurcations, or custom continuation code). For the stochastic part, we might employ Gillespie algorithms if we treat some interactions as reactions, or just SDE integrators. The code and model will be made available openly to allow other researchers to input their data and see if the model reproduces it, fostering collaborative refinement.

Conclusion

In this proposal, we have expanded the GASLIT-AF framework into a comprehensive mathematical model for chronic multi-system syndromes. By formalizing the state space Ψ and constructing a set of nonlinear delayed differential equations, we capture the complex web of interactions underlying conditions like ME/CFS, Long COVID, POTS, hEDS, fibromyalgia, and MCAS. Each parameter (γ, Λ, Ω, σ, Θ, Ξ) has been defined with a clear biophysical interpretation, allowing us to map clinical or molecular data onto the model. Through stability and bifurcation analysis, we account for the emergence of pathological attractors – the self-perpetuating fatigue/pain states that characterize these illnesses – and show how they can arise when system stability is compromised. We integrated concepts from information theory (mutual information, entropy) and physiology (coherence, complexity) to connect the mathematics with observable hallmarks of disease (like loss of complexity and increased entropy production in chronic illness ￼ ￼). The framework was further enriched by considering stochastic fluctuations, network topology, control interventions, and the geometric structure of parameter space, ensuring that our model is robust, testable, and insightful from multiple perspectives.

Crucially, we demonstrated with real examples (KLK15 variant, IDO trap) how the abstract model corresponds to tangible biological mechanisms and can incorporate new findings ￼ ￼. This positions the GASLIT-AF model not as a purely theoretical construct, but as a living hypothesis that evolves with biomedical research. The end result is a systems biology modeling proposal that is well-defined mathematically yet remains connected to clinical reality. By simulating and analyzing this model, researchers and clinicians can gain a deeper understanding of why these syndromes are so persistent and hard to treat, and importantly, derive new ideas for interventions (for example, identifying which feedback loop to break, or which parameter to modulate through therapy).

The GASLIT-AF model embodies the principle that “a disease is rarely a consequence of an abnormality in a single gene, but reflects perturbations of a complex network” ￼. We have taken this principle to the quantitative level, laying out equations and methods to rigorously explore those network perturbations. If successful, this approach could unify disparate syndromes under common dynamical principles, explain the individual variability via parameter differences, and guide personalized medicine by indicating each patient’s position in state space and the most efficient path to restore stability. This represents a novel, mathematically-grounded approach to conditions that have long been enigmatic, hopefully leading to better diagnostics and treatments grounded in an understanding of the system as a whole.

Sources: The concepts and data referenced in this proposal draw upon interdisciplinary literature, including systems biology studies of disease attractors ￼, network medicine perspectives ￼, known hypotheses in ME/CFS research ￼, recent genetic findings in hEDS ￼, and theoretical work on complexity in physiological systems ￼ ￼. These sources are cited inline to support the feasibility and relevance of the modeling approach. Each citation (e.g. ￼) corresponds to findings that have informed our model design or expectations, ensuring that the framework is built upon established scientific observations.